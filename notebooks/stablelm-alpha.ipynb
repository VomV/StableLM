{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4weyZUFfgUlD"
      },
      "source": [
        "# StableLM-Alpha\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Stability-AI/StableLM//blob/main/notebooks/stablelm-alpha.ipynb)\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/Stability-AI/StableLM/main/assets/mascot.png?token=GHSAT0AAAAAABWTZAV7EFSADKXWO3HDNKPYZBZ6Z7A\"/>\n",
        "\n",
        "This notebook is designed to let you quickly generate text with the latest StableLM models (**StableLM-Alpha**) using Hugging Face's `transformers` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8xicyuk_Ezuw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V1Da2YDX71IF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /home/vivek/anaconda3/envs/transformers/lib/python3.10/site-packages (23.1)\n",
            "Requirement already satisfied: accelerate in /home/vivek/anaconda3/envs/transformers/lib/python3.10/site-packages (0.18.0)\n",
            "Requirement already satisfied: bitsandbytes in /home/vivek/anaconda3/envs/transformers/lib/python3.10/site-packages (0.38.1)\n",
            "Requirement already satisfied: transformers in /home/vivek/.local/lib/python3.10/site-packages (4.27.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/vivek/anaconda3/envs/transformers/lib/python3.10/site-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/vivek/.local/lib/python3.10/site-packages (from accelerate) (22.0)\n",
            "Requirement already satisfied: psutil in /home/vivek/.local/lib/python3.10/site-packages (from accelerate) (5.9.4)\n",
            "Requirement already satisfied: pyyaml in /home/vivek/anaconda3/envs/transformers/lib/python3.10/site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /home/vivek/.local/lib/python3.10/site-packages (from accelerate) (2.0.0)\n",
            "Requirement already satisfied: filelock in /home/vivek/.local/lib/python3.10/site-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/vivek/.local/lib/python3.10/site-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/vivek/.local/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /home/vivek/.local/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/vivek/.local/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/vivek/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vivek/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: sympy in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (3.0)\n",
            "Requirement already satisfied: jinja2 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /home/vivek/.local/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /home/vivek/anaconda3/envs/transformers/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (65.5.0)\n",
            "Requirement already satisfied: wheel in /home/vivek/anaconda3/envs/transformers/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (0.37.1)\n",
            "Requirement already satisfied: cmake in /home/vivek/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.26.1)\n",
            "Requirement already satisfied: lit in /home/vivek/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/vivek/.local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/vivek/.local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vivek/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/vivek/.local/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/vivek/anaconda3/envs/transformers/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/vivek/.local/lib/python3.10/site-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip\n",
        "!pip install accelerate bitsandbytes transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "sSifeGXKlIgY"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "def hr(): display(Markdown('---'))\n",
        "def cprint(msg: str, color: str = \"blue\", **kwargs) -> str:\n",
        "    if color == \"blue\": print(\"\\033[34m\" + msg + \"\\033[0m\", **kwargs)\n",
        "    elif color == \"red\": print(\"\\033[31m\" + msg + \"\\033[0m\", **kwargs)\n",
        "    elif color == \"green\": print(\"\\033[32m\" + msg + \"\\033[0m\", **kwargs)\n",
        "    elif color == \"yellow\": print(\"\\033[33m\" + msg + \"\\033[0m\", **kwargs)\n",
        "    elif color == \"purple\": print(\"\\033[35m\" + msg + \"\\033[0m\", **kwargs)\n",
        "    elif color == \"cyan\": print(\"\\033[36m\" + msg + \"\\033[0m\", **kwargs)\n",
        "    else: raise ValueError(f\"Invalid info color: `{color}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "dQZCeE-ujdzW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mUsing `stabilityai/stablelm-tuned-alpha-7b`\u001b[0m\n",
            "\u001b[34mLoading with: `torch_dtype='float16', load_in_8bit=False, device_map='auto'`\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8112a35b9cfb4e49861aa70095893a16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/264 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ea4445c4c4e47d6861ba78d72b94bc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1b579504667457c9eff27b429dfd7c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d20561f206474d6d8dd43a4bcda32b77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6fc7027de7445fda9d0619c895c56e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55cb35f9fdb54f20bee73ce3c4dc65ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bedc50186fb247049612c5b9f0446522",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/9.78G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b07205f398a84aeaa3ab28de31636180",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/9.77G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1140046aa7144098affcfaa1d7c3d1b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/9.75G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16c4ae61a44246858e58704b9bf2fa25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.45G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "266d8c0ebadd4f40a4663d498a796389",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5720246335e4481ca48b974b808f4bf0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Pick Your Model\n",
        "#@markdown Refer to Hugging Face docs for more information the parameters below: https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained\n",
        "\n",
        "# Choose model name\n",
        "model_name = \"stabilityai/stablelm-tuned-alpha-7b\" #@param [\"stabilityai/stablelm-base-alpha-7b\", \"stabilityai/stablelm-tuned-alpha-7b\", \"stabilityai/stablelm-base-alpha-3b\", \"stabilityai/stablelm-tuned-alpha-3b\"]\n",
        "\n",
        "cprint(f\"Using `{model_name}`\", color=\"blue\")\n",
        "\n",
        "# Select \"big model inference\" parameters\n",
        "torch_dtype = \"float16\" #@param [\"float16\", \"bfloat16\", \"float\"]\n",
        "load_in_8bit = False #@param {type:\"boolean\"}\n",
        "device_map = \"auto\"\n",
        "\n",
        "cprint(f\"Loading with: `{torch_dtype=}, {load_in_8bit=}, {device_map=}`\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=getattr(torch, torch_dtype),\n",
        "    load_in_8bit=load_in_8bit,\n",
        "    # device_map=device_map,\n",
        "    offload_folder=\"./offload\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "P01Db-SVwtPO",
        "outputId": "69ab8b7e-2981-4a55-b03e-dc4f1545f979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mSampling with: `max_new_tokens=64, temperature=0.5, top_k=0, top_p=0.9, do_sample=True`\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "\"LayerNormKernelImpl\" not implemented for 'Half'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m inputs\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Generate\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     22\u001b[0m   \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m     23\u001b[0m   max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m     24\u001b[0m   temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m     25\u001b[0m   top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m     26\u001b[0m   top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m     27\u001b[0m   do_sample\u001b[39m=\u001b[39;49mdo_sample,\n\u001b[1;32m     28\u001b[0m   pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[39m# Extract out only the completion tokens\u001b[39;00m\n\u001b[1;32m     32\u001b[0m completion_tokens \u001b[39m=\u001b[39m tokens[\u001b[39m0\u001b[39m][inputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m):]\n",
            "File \u001b[0;32m~/anaconda3/envs/transformers/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1452\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1445\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1446\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1447\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1448\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1449\u001b[0m     )\n\u001b[1;32m   1451\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1452\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1453\u001b[0m         input_ids,\n\u001b[1;32m   1454\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1455\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1456\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1457\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1458\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1459\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1460\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1461\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1462\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1463\u001b[0m     )\n\u001b[1;32m   1465\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1466\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2468\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2465\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2467\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2468\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2469\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2470\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2471\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2472\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2473\u001b[0m )\n\u001b[1;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2476\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:640\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    638\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 640\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m    641\u001b[0m     input_ids,\n\u001b[1;32m    642\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    643\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    644\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    645\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    646\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    647\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    648\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    649\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    652\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    653\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_out(hidden_states)\n",
            "File \u001b[0;32m~/anaconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:533\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    526\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    527\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    528\u001b[0m         hidden_states,\n\u001b[1;32m    529\u001b[0m         attention_mask,\n\u001b[1;32m    530\u001b[0m         head_mask[i],\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    534\u001b[0m         hidden_states,\n\u001b[1;32m    535\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    536\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    537\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    538\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    539\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    540\u001b[0m     )\n\u001b[1;32m    541\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    542\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:319\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    310\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    311\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    317\u001b[0m ):\n\u001b[1;32m    318\u001b[0m     attention_layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[0;32m--> 319\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states),\n\u001b[1;32m    320\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    321\u001b[0m         layer_past\u001b[39m=\u001b[39mlayer_past,\n\u001b[1;32m    322\u001b[0m         head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m    323\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    324\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    326\u001b[0m     attn_output \u001b[39m=\u001b[39m attention_layer_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     outputs \u001b[39m=\u001b[39m attention_layer_outputs[\u001b[39m1\u001b[39m:]\n",
            "File \u001b[0;32m~/anaconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    191\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
            "File \u001b[0;32m~/anaconda3/envs/transformers/lib/python3.10/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"LayerNormKernelImpl\" not implemented for 'Half'"
          ]
        }
      ],
      "source": [
        "#@title Generate Text \n",
        "#@markdown <b>Note: The model response is colored in green</b>\n",
        "\n",
        "prompt = \"There once was a parrot named Alph who sang\" #@param {type:\"string\"}\n",
        "\n",
        "# Sampling args\n",
        "max_new_tokens = 64 #@param {type:\"slider\", min:32.0, max:3072.0, step:32}\n",
        "temperature = 0.5 #@param {type:\"slider\", min:0.0, max:1.25, step:0.05}\n",
        "top_k = 0 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "top_p = 0.9 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "do_sample = True #@param {type:\"boolean\"}\n",
        "\n",
        "cprint(f\"Sampling with: `{max_new_tokens=}, {temperature=}, {top_k=}, {top_p=}, {do_sample=}`\")\n",
        "hr()\n",
        "\n",
        "# Create `generate` inputs\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs.to(model.device)\n",
        "\n",
        "# Generate\n",
        "tokens = model.generate(\n",
        "  **inputs,\n",
        "  max_new_tokens=max_new_tokens,\n",
        "  temperature=temperature,\n",
        "  top_k=top_k,\n",
        "  top_p=top_p,\n",
        "  do_sample=do_sample,\n",
        "  pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# Extract out only the completion tokens\n",
        "completion_tokens = tokens[0][inputs['input_ids'].size(1):]\n",
        "completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
        "\n",
        "# Display\n",
        "print(prompt, end=\"\")\n",
        "cprint(completion, color=\"green\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rIZm5uwaQLa4"
      },
      "source": [
        "## License (Apache 2.0)\n",
        "\n",
        "Copyright (c) 2023 by [StabilityAI LTD](https://stability.ai/)\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
